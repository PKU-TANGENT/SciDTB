Phrase-based models 
directly trained on mix-of-domain corpora 
can be sub-optimal. <S>
In this paper we equip phrase-based models with a latent domain variable 
and present a novel method 
for adapting them to an in-domain task 
represented by a seed corpus . <S>
We derive an EM algorithm 
which alternates between inducing domain-focused phrase pair estimates , and weights for mix-domain sentence pairs 
reflecting their relevance for the in-domain task . <S>
By embedding our latent domain phrase model in a sentence-level model 
and training the two in tandem , 
we are able to adapt all core translation components together - phrase , lexical and reordering . <S>
We show experiments on weighing sentence pairs for relevance as well as adapting phrase-based models , 
showing significant performance improvement in both tasks . <S>
